import argparse
import csv
import os
import openai

# Function to generate the initial part of the evaluation prompt
def generate_initial_prompt():
    return (
        '''
The goal of this task is to rate the generated cyber grooming chat. Please take the time to fully understand the chat and the following question. Make sure to answer all sub questions without any exceptions. 
Make sure to decrease your rating based on the types and number of grammar errors in each chat. Give only an evaluation score without any additional explanation.
'''

    )

# Function to generate the evaluation question
def generate_question_prompt():
    return (
        '''
        Question: How grammatically correct is the text of the chat? (on a scale of 1-5, with 1 being the lowest). Consider the following:\n
        Punctuation.\n
        Word choice, subject-verb agreement.\n
        Proper syntax.\n
        Are the sentences well-constructed and convey the intended meaning.
        '''
    )

# Function to evaluate chats using text-davinci-003
def evaluate_chats(chats, model_name):
    openai.api_key = os.getenv("OPENAI_API_KEY")

    evaluations = []
    for chat in chats:
        # Send the initial part of the prompt
        initial_prompt = generate_initial_prompt()
        openai.Completion.create(
            model="text-davinci-003",
            prompt=initial_prompt,
            max_tokens=512,
            temperature=0.7
        )
        
        # Send the chat
        openai.Completion.create(
            model="text-davinci-003",
            prompt=initial_prompt,
            max_tokens=512,
            temperature=0.7
      
        )

        # Send the evaluation question and get the response
        question_prompt = generate_question_prompt()
        response = openai.Completion.create(
          model="text-davinci-003",
            prompt=initial_prompt,
            max_tokens=512,
            temperature=0.7
         
        )
        score = response.choices[0].text.strip()
        evaluations.append(score)
    
    return evaluations

# Function to read chats from CSV
def read_chats_from_csv(file_path):
    chats = []
    with open(file_path, 'r', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile)
        next(reader)  # Skip header
        for row in reader:
            chats.append(row[0])
    return chats

# Function to save evaluations to CSV
def save_evaluations_to_csv(evaluations, model_name):
    with open(f"{model_name}_evaluations.csv", "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Grammatical Correctness"])
        writer.writerows([[evaluation] for evaluation in evaluations])

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate grammatical correctness of chats generated by different LLMs")
    parser.add_argument("--model", type=str, choices=["gpt-3.5-turbo", "Wizard-Vicuna-uncensored"], help="Language model to evaluate")
    parser.add_argument("--chats_file", type=str, help="CSV file containing the chats to be evaluated")

    args = parser.parse_args()

    chats = read_chats_from_csv(args.chats_file)
    evaluations = evaluate_chats(chats)
    save_evaluations_to_csv(evaluations, "Grammatical Correctness")